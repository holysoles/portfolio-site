title: Wrangling LLM Web Crawlers
tags:
  - highlights
  - security
  - go
  - k8s
body:
  - section_title: "Background"
    text: There's been a lot of media coverage recently around LLM data scraping, particularly of [resources in the open source community](https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/). 
      These scrapers often ignore robots.txt, leading to inspiration for tools like [Nepenthes](https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt), 
      [Cloudflare's recent implementation of the same idea](https://www.theverge.com/news/634345/cloudflare-ai-labyrinth-web-scraping-bots-training-data#comments), [iocaine](https://iocaine.madhouse-project.org/), 
      [Anubis](https://anubis.techaro.lol/), and many others. As mentioned in most of those sources, the amount of traffic generated by these bots, particularly on popular open source websites, has been concerning, and 
      has increased hosting costs. The current scheme of training AI models will continue to need more data, and as more competitors enter the space, it seems that this won't be slowing down any time soon.
  - text: While these tools are a great take to try to address this problem, most have not developed a simple method for deploying and integrating with your existing networking solution. I wanted to make it as easy as 
      possible to integrate these tools, make sure they only get applied to undesired web crawlers.
  - section_title: Writing a Traefik Plugin
    text: As a Traefik user for my Kubernetes cluster, I've long been a user of the [CrowdSec Traefik Plugin](https://github.com/maxlerebourg/crowdsec-bouncer-traefik-plugin), which is effectively a native fail2ban 
      solution, with the additive benefit of ban-list sharing among users. This has been a good solution for bad actors in general, as IPs that submit requests that match known vulnerability heuristics will be blocked. 
      Knowing this type of request handling was possible through a Traefik plugin, led me to follow the same path for this project.
  - text: Traefik Middleware plugins are golang modules that implement a `http.Handler` type and `ServeHTTP` interface, and either modify the request or . These are indexed by Traefik by searching GitHub, running tests 
      against the package, and if a user has specified it for use in their Traefik instance static configuration, it will be loaded at runtime through [the Yaegi interpreter](https://github.com/traefik/yaegi). Yaegi has 
      worked great for me while working on this project, though notably the `unsafe` stdlib package cannot be loaded. While I didn't plan to use this package directly, I was hoping to use [zerolog](https://github.com/rs/zerolog) 
      for logging, so that logging could be done in JSON, and match Traefik's native logs easily. However, zerolog ends up using `unsafe` as a dependency. As a result, I implemented a custom log handler that, for the time 
      being, writes simple strings to Stdout/Stderr.
  - text: The foundation for the plugin is the [ai-robots.txt](https://github.com/ai-robots-txt/ai.robots.txt) project, a community maintained list of LLM bot user-agents, their operators, and additional information. The 
      plugin caches this list on a regular basis and uses it in two ways. The first is to generate a `robots.txt` file dynamically when requested for an ingressroute that implements the middleware. This file is used for the 
      Robots Exclusion Protocol, meant to tell bots which resources on the site, if any, they are allowed to visit. However, its been well documented that many of these LLM data scraping bots do not respect the contents of this 
      file.
  - text: If a request is **not** to `/robots.txt` and is from one of these bot's user-agents, then the request can be simplied logged, rejected with a 403 error, or proxied to a custom service. The proxy feature is meant to 
      forward the request to a "tarpit" like app such as [Nepenthes](https://zadzmo.org/code/nepenthes/), [iocaine](https://iocaine.madhouse-project.org/), and others. The proxy is configured to be unbuffered such that a tarpit 
      that intentionally trickles information to the client does not impact performance.
  - text: The combination of these two features ensures that only bots that ignore the Robots Exclusion Protocol are being acted upon. Users can also configure the application with their own custom robots.txt index, if they want 
      to allow specific bots, or block even more user agents that may not be encompassed in the list.
  - text: The plugin is available through the [Traefik Plugin Catalog](https://plugins.traefik.io/plugins), and a configuration and deployment guide can be found on 
      [the project's README](https://github.com/holysoles/bot-wrangler-traefik-plugin?tab=readme-ov-file#bot-wrangler-traefik-plugin).
  - section_title: Future Plans
    text: While this plugin addresses controlling traffic when the request from a web crawler provides an accurate user-agent, it does not handle the case where it might be forged or omitted from request headers. To address 
      this I'd like to work on a feature where requests can be evaluated by alternative methods, such as needing to complete a Proof-of-Work challenge like [Anubis](https://anubis.techaro.lol/), or [Altcha](https://altcha.org/)
  - text: I also plan to improve the logging feature to properly log in JSON so that the logs can be ingested into tools like Loki and be queried easier.